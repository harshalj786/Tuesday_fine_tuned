{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e3f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML_PROJECTS\\TUESDAY\\gpu_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForCausalLM, \n",
    "    TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset as HFDataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e05bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCompletionOnlyCollator: \n",
    "    \"\"\"\n",
    "    Custom data collator that only computes loss on the completion/response part.\n",
    "    This replaces DataCollatorForCompletionOnlyLM for older trl versions.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, response_template=\"<|assistant|>\", mlm=False, max_length=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.response_template = response_template\n",
    "        self.response_template_ids = tokenizer.encode(\n",
    "            response_template, add_special_tokens=False\n",
    "        )\n",
    "        self.mlm = mlm\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        # Handle both list of dicts and dict of lists\n",
    "        if isinstance(examples, list):\n",
    "            if examples and isinstance(examples[0], dict) and \"input_ids\" not in examples[0]:\n",
    "                texts = []\n",
    "                for example in examples:\n",
    "                    if isinstance(example, dict):\n",
    "                        if \"text\" in example:\n",
    "                            texts.append(example[\"text\"] )\n",
    "                        else:\n",
    "                            raise ValueError(\"Example dictionary missing 'text' field required for tokenization.\")\n",
    "                    else:\n",
    "                        texts.append(str(example))\n",
    "                batch = self.tokenizer(\n",
    "                    texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "            else:\n",
    "                batch = self.tokenizer.pad(\n",
    "                    examples,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "        else:\n",
    "            batch = examples\n",
    "        \n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        \n",
    "        # For each sequence, mask everything before the response template\n",
    "        for i in range(len(labels)):\n",
    "            input_ids = batch[\"input_ids\"][i].tolist()\n",
    "            \n",
    "            # Find the response template position\n",
    "            response_start = self._find_response_start(input_ids)\n",
    "            \n",
    "            if response_start != -1:\n",
    "                # Mask everything before the response (set to -100)\n",
    "                labels[i, :response_start] = -100\n",
    "            \n",
    "            # Also mask padding tokens\n",
    "            if self.tokenizer.pad_token_id is not None: \n",
    "                labels[i, labels[i] == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "    \n",
    "    def _find_response_start(self, input_ids):\n",
    "        \"\"\"Find where the response template starts in the input.\"\"\"\n",
    "        template_len = len(self.response_template_ids)\n",
    "        for i in range(len(input_ids) - template_len + 1):\n",
    "            if input_ids[i:i + template_len] == self.response_template_ids:\n",
    "                return i + template_len\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147f8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths - FIXED:  no space in filename\n",
    "    DATA_PATH = r\"D:\\ML_PROJECTS\\TUESDAY\\Tuesday_bot\\cleaned.csv\"\n",
    "    OUTPUT_DIR = \"./models\"\n",
    "    \n",
    "    # Model names\n",
    "    CLASSIFIER_MODEL = \"distilbert-base-uncased\"\n",
    "    POLICY_MODEL = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "    \n",
    "    # Training hyperparameters - optimized for 12GB VRAM\n",
    "    CLASSIFIER_BATCH_SIZE = 16\n",
    "    CLASSIFIER_EPOCHS = 5\n",
    "    CLASSIFIER_LR = 2e-5\n",
    "    CLASSIFIER_MAX_LEN = 128\n",
    "    \n",
    "    POLICY_BATCH_SIZE = 1\n",
    "    POLICY_GRAD_ACCUM = 8\n",
    "    POLICY_EPOCHS = 3\n",
    "    POLICY_LR = 1e-4\n",
    "    POLICY_MAX_LEN = 512\n",
    "    \n",
    "    # LoRA settings\n",
    "    LORA_R = 16\n",
    "    LORA_ALPHA = 32\n",
    "    LORA_DROPOUT = 0.05\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65db7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Aggressively clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda. synchronize()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch. cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"GPU Memory:  {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba78a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedMentalStateModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved multi-task classifier with:\n",
    "    - Task-specific attention\n",
    "    - Residual connections\n",
    "    - Better regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, num_emotions, num_intents, num_risk, model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        h = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Store config for reconstruction\n",
    "        self.num_emotions = num_emotions\n",
    "        self.num_intents = num_intents\n",
    "        self.num_risk = num_risk\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Shared layers with residual\n",
    "        self.shared1 = nn.Linear(h, h)\n",
    "        self.shared2 = nn.Linear(h, h)\n",
    "        self.layer_norm = nn.LayerNorm(h)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Task-specific heads with intermediate layers\n",
    "        self.emotion_hidden = nn.Linear(h, h // 2)\n",
    "        self.emotion_head = nn.Linear(h // 2, num_emotions)\n",
    "        \n",
    "        self.intent_hidden = nn.Linear(h, h // 4)\n",
    "        self.intent_head = nn.Linear(h // 4, num_intents)\n",
    "        \n",
    "        self.risk_hidden = nn.Linear(h, h // 4)\n",
    "        self.risk_head = nn.Linear(h // 4, num_risk)\n",
    "        \n",
    "        self.intensity_hidden = nn.Linear(h, h // 4)\n",
    "        self.intensity_head = nn.Linear(h // 4, 1)\n",
    "        \n",
    "        # Confidence estimation\n",
    "        self.confidence_head = nn.Linear(h, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Attention-weighted pooling\n",
    "        hidden = out.last_hidden_state\n",
    "        attention_weights = attention_mask.unsqueeze(-1).float()\n",
    "        pooled = (hidden * attention_weights).sum(dim=1) / attention_weights.sum(dim=1).clamp(min=1e-9)\n",
    "        \n",
    "        # Shared representation with residual\n",
    "        z = self.dropout(F.gelu(self.shared1(pooled)))\n",
    "        z = self.layer_norm(z + self.dropout(F.gelu(self.shared2(z))))\n",
    "        \n",
    "        # Task-specific outputs\n",
    "        emotion_h = self.dropout(F.gelu(self.emotion_hidden(z)))\n",
    "        intent_h = self.dropout(F.gelu(self.intent_hidden(z)))\n",
    "        risk_h = self.dropout(F.gelu(self.risk_hidden(z)))\n",
    "        intensity_h = self.dropout(F.gelu(self.intensity_hidden(z)))\n",
    "        \n",
    "        return {\n",
    "            \"emotion\": self.emotion_head(emotion_h),\n",
    "            \"intent\": self.intent_head(intent_h),\n",
    "            \"risk\": self.risk_head(risk_h),\n",
    "            \"intensity\": torch.sigmoid(self.intensity_head(intensity_h)).squeeze(-1),\n",
    "            \"confidence\": torch.sigmoid(self.confidence_head(z)).squeeze(-1),\n",
    "            \"semantic_vector\": z\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6711ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 1: Data Preprocessing\n",
      "================================================================================\n",
      "Dataset size: 61682\n",
      "Number of emotions: 32\n",
      "Emotions:  ['afraid', 'angry', 'annoyed', 'anticipating', 'anxious', 'apprehensive', 'ashamed', 'caring', 'confident', 'content', 'devastated', 'disappointed', 'disgusted', 'embarrassed', 'excited', 'faithful', 'furious', 'grateful', 'guilty', 'hopeful', 'impressed', 'jealous', 'joyful', 'lonely', 'nostalgic', 'prepared', 'proud', 'sad', 'sentimental', 'surprised', 'terrified', 'trusting']\n",
      "Risk distribution: {0: 61620, 1: 36, 2: 26}\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 1: Data Preprocessing\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df = pd.read_csv(config.DATA_PATH)\n",
    "\n",
    "def clean_user_text(text):\n",
    "    \"\"\"Clean and normalize user text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'^\\s*customer\\s*:\\s*', '', text, flags=re.I)\n",
    "    text = re.split(r'\\bagent\\s*:', text, flags=re.I)[0]\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"clf_text\"] = df[\"empathetic_dialogues\"].apply(clean_user_text)\n",
    "\n",
    "# Enhanced filtering\n",
    "df = df[df[\"labels\"]. notna()]\n",
    "df = df[df[\"labels\"]. str.strip() != \"\"]\n",
    "df = df[df[\"labels\"].str.split().str.len() >= 3]\n",
    "df = df[df[\"clf_text\"].str.len() >= 10]\n",
    "df = df. reset_index(drop=True)\n",
    "\n",
    "# Emotion filtering\n",
    "emotion_counts = df[\"emotion\"].value_counts()\n",
    "valid_emotions = emotion_counts[emotion_counts >= 15].index\n",
    "df = df[df[\"emotion\"]. isin(valid_emotions)].reset_index(drop=True)\n",
    "\n",
    "# Create emotion labels\n",
    "df[\"label\"] = df[\"emotion\"].astype(\"category\").cat.codes\n",
    "id2label = dict(enumerate(df[\"emotion\"].astype(\"category\").cat.categories))\n",
    "label2id = {v:  k for k, v in id2label.items()}\n",
    "NUM_EMOTIONS = len(id2label)\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "print(f\"Number of emotions: {NUM_EMOTIONS}\")\n",
    "print(f\"Emotions:  {list(id2label.values())}\")\n",
    "\n",
    "# Enhanced intensity mapping\n",
    "INTENSITY_MAP = {\n",
    "    \"sad\": 0.8, \"angry\": 0.9, \"anxious\": 0.85, \"fear\": 0.9,\n",
    "    \"lonely\": 0.8, \"depressed\": 0.95, \"hopeless\": 0.95,\n",
    "    \"neutral\": 0.3, \"content\": 0.25,\n",
    "    \"happy\": 0.2, \"excited\": 0.4, \"grateful\": 0.3, \"proud\": 0.4,\n",
    "    \"surprised\": 0.5, \"confused\": 0.6,\n",
    "}\n",
    "df[\"intensity\"] = df[\"emotion\"].apply(lambda e: INTENSITY_MAP. get(e. lower(), 0.5))\n",
    "\n",
    "# Enhanced intent inference\n",
    "def infer_intent(text):\n",
    "    t = text.lower()\n",
    "    if any(x in t for x in [\"what should i\", \"how do i\", \"how can i\", \"can you help\", \n",
    "                            \"any advice\", \"tips\", \"suggestions\", \"recommend\", \"what would you\"]):\n",
    "        return \"advice\"\n",
    "    if any(x in t for x in [\"just wanted to\", \"needed to vent\", \"i feel like\", \"honestly\", \n",
    "                            \"ngl\", \"lowkey\", \"i just\", \"ugh\", \"i hate\", \"so tired of\"]):\n",
    "        return \"venting\"\n",
    "    return \"validation\"\n",
    "\n",
    "INTENT_MAP = {\"venting\": 0, \"advice\": 1, \"validation\": 2}\n",
    "df[\"intent_id\"] = df[\"clf_text\"]. apply(lambda x: INTENT_MAP[infer_intent(x)])\n",
    "NUM_INTENTS = len(INTENT_MAP)\n",
    "\n",
    "# Enhanced risk inference\n",
    "def infer_risk(text):\n",
    "    t = text.lower()\n",
    "    high_risk_keywords = [\n",
    "        \"kill myself\", \"end it all\", \"can't go on\", \"want to die\", \"suicide\",\n",
    "        \"better off dead\", \"end my life\", \"hurt myself\", \"self harm\", \"cutting\"\n",
    "    ]\n",
    "    if any(x in t for x in high_risk_keywords):\n",
    "        return 2\n",
    "    medium_risk_keywords = [\n",
    "        \"hopeless\", \"worthless\", \"give up\", \"no point\", \"don't want to be here\",\n",
    "        \"nothing matters\", \"can't take it anymore\", \"so done\", \"hate myself\"\n",
    "    ]\n",
    "    if any(x in t for x in medium_risk_keywords):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df[\"risk_id\"] = df[\"clf_text\"].apply(infer_risk)\n",
    "NUM_RISK = 3\n",
    "\n",
    "print(f\"Risk distribution: {df['risk_id']. value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c300993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: Training Mental State Classifier\n",
      "================================================================================\n",
      "Using device: cuda\n",
      "GPU Memory:  0.00GB allocated, 0.00GB reserved\n",
      "\n",
      "Training Mental State Model...\n",
      "Epoch 1/5\n",
      "  Train Loss: 3.0039, Train Acc: 0.2348\n",
      "  Val Loss:  2.6884, Val Acc: 0.3112, Risk Acc: 0.9992\n",
      "  âœ“ Saved best model (Val Acc: 0.3112)\n",
      "Epoch 2/5\n",
      "  Train Loss: 2.5288, Train Acc: 0.3540\n",
      "  Val Loss:  2.6497, Val Acc: 0.3341, Risk Acc: 0.9994\n",
      "  âœ“ Saved best model (Val Acc: 0.3341)\n",
      "Epoch 3/5\n",
      "  Train Loss: 2.2931, Train Acc: 0.4227\n",
      "  Val Loss:  2.6931, Val Acc: 0.3323, Risk Acc: 0.9994\n",
      "Epoch 4/5\n",
      "  Train Loss: 2.1090, Train Acc: 0.4807\n",
      "  Val Loss:  2.7240, Val Acc: 0.3346, Risk Acc: 0.9994\n",
      "  âœ“ Saved best model (Val Acc: 0.3346)\n",
      "Epoch 5/5\n",
      "  Train Loss: 2.0191, Train Acc: 0.5117\n",
      "  Val Loss:  2.7557, Val Acc: 0.3302, Risk Acc: 0.9994\n",
      "\n",
      "Best Validation Accuracy: 0.3346\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: Training Mental State Classifier\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.CLASSIFIER_MODEL)\n",
    "\n",
    "class MentalStateDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.texts = df[\"clf_text\"].tolist()\n",
    "        self.emotion = df[\"label\"].tolist()\n",
    "        self.intent = df[\"intent_id\"].tolist()\n",
    "        self.risk = df[\"risk_id\"].tolist()\n",
    "        self.intensity = df[\"intensity\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"emotion\": torch.tensor(self.emotion[idx], dtype=torch.long),\n",
    "            \"intent\": torch.tensor(self.intent[idx], dtype=torch.long),\n",
    "            \"risk\": torch.tensor(self.risk[idx], dtype=torch.long),\n",
    "            \"intensity\": torch.tensor(self.intensity[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Stratified split\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_ds = MentalStateDataset(train_df, tokenizer, config.CLASSIFIER_MAX_LEN)\n",
    "val_ds = MentalStateDataset(val_df, tokenizer, config.CLASSIFIER_MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=config.CLASSIFIER_BATCH_SIZE, \n",
    "    shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=config.CLASSIFIER_BATCH_SIZE, \n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# Initialize model\n",
    "model = ImprovedMentalStateModel(\n",
    "    NUM_EMOTIONS, NUM_INTENTS, NUM_RISK, config.CLASSIFIER_MODEL\n",
    ").to(config.DEVICE)\n",
    "\n",
    "# Loss functions\n",
    "loss_emotion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss_intent = nn.CrossEntropyLoss()\n",
    "risk_weights = torch.tensor([1.0, 3.0, 5.0]).to(config.DEVICE)\n",
    "loss_risk = nn.CrossEntropyLoss(weight=risk_weights)\n",
    "loss_intensity = nn.MSELoss()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.CLASSIFIER_LR, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config.CLASSIFIER_EPOCHS * len(train_loader)\n",
    ")\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_emotion = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(config.DEVICE) for k, v in batch.items()}\n",
    "        out = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        \n",
    "        loss = (\n",
    "            1.0 * loss_emotion(out[\"emotion\"], batch[\"emotion\"]) +\n",
    "            0.5 * loss_intent(out[\"intent\"], batch[\"intent\"]) +\n",
    "            1.5 * loss_risk(out[\"risk\"], batch[\"risk\"]) +\n",
    "            0.3 * loss_intensity(out[\"intensity\"], batch[\"intensity\"])\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct_emotion += (out[\"emotion\"].argmax(dim=1) == batch[\"emotion\"]).sum().item()\n",
    "        total += batch[\"emotion\"].size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct_emotion / total\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_emotion = 0\n",
    "    correct_risk = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(config.DEVICE) for k, v in batch.items()}\n",
    "            out = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            \n",
    "            loss = (\n",
    "                loss_emotion(out[\"emotion\"], batch[\"emotion\"]) +\n",
    "                0.5 * loss_intent(out[\"intent\"], batch[\"intent\"]) +\n",
    "                1.5 * loss_risk(out[\"risk\"], batch[\"risk\"]) +\n",
    "                0.3 * loss_intensity(out[\"intensity\"], batch[\"intensity\"])\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct_emotion += (out[\"emotion\"].argmax(dim=1) == batch[\"emotion\"]).sum().item()\n",
    "            correct_risk += (out[\"risk\"].argmax(dim=1) == batch[\"risk\"]).sum().item()\n",
    "            total += batch[\"emotion\"].size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct_emotion / total, correct_risk / total\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining Mental State Model...\")\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(config.CLASSIFIER_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    val_loss, val_acc, risk_acc = validate(model, val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config.CLASSIFIER_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:  {val_loss:.4f}, Val Acc: {val_acc:.4f}, Risk Acc: {risk_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'id2label': id2label,\n",
    "            'label2id': label2id,\n",
    "            'config': {\n",
    "                'num_emotions': NUM_EMOTIONS,\n",
    "                'num_intents': NUM_INTENTS,\n",
    "                'num_risk': NUM_RISK,\n",
    "                'model_name': config.CLASSIFIER_MODEL\n",
    "            }\n",
    "        }, f\"{config.OUTPUT_DIR}/mental_state_model_best.pth\")\n",
    "        print(f\"  âœ“ Saved best model (Val Acc: {val_acc:.4f})\")\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34fe5e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3:  Generating Policy Training Data\n",
      "================================================================================\n",
      "Loading best classifier model...\n",
      "Generating mental state predictions...\n",
      "Processing 0/61682\n",
      "Processing 500/61682\n",
      "Processing 1000/61682\n",
      "Processing 1500/61682\n",
      "Processing 2000/61682\n",
      "Processing 2500/61682\n",
      "Processing 3000/61682\n",
      "Processing 3500/61682\n",
      "Processing 4000/61682\n",
      "Processing 4500/61682\n",
      "Processing 5000/61682\n",
      "Processing 5500/61682\n",
      "Processing 6000/61682\n",
      "Processing 6500/61682\n",
      "Processing 7000/61682\n",
      "Processing 7500/61682\n",
      "Processing 8000/61682\n",
      "Processing 8500/61682\n",
      "Processing 9000/61682\n",
      "Processing 9500/61682\n",
      "Processing 10000/61682\n",
      "Processing 10500/61682\n",
      "Processing 11000/61682\n",
      "Processing 11500/61682\n",
      "Processing 12000/61682\n",
      "Processing 12500/61682\n",
      "Processing 13000/61682\n",
      "Processing 13500/61682\n",
      "Processing 14000/61682\n",
      "Processing 14500/61682\n",
      "Processing 15000/61682\n",
      "Processing 15500/61682\n",
      "Processing 16000/61682\n",
      "Processing 16500/61682\n",
      "Processing 17000/61682\n",
      "Processing 17500/61682\n",
      "Processing 18000/61682\n",
      "Processing 18500/61682\n",
      "Processing 19000/61682\n",
      "Processing 19500/61682\n",
      "Processing 20000/61682\n",
      "Processing 20500/61682\n",
      "Processing 21000/61682\n",
      "Processing 21500/61682\n",
      "Processing 22000/61682\n",
      "Processing 22500/61682\n",
      "Processing 23000/61682\n",
      "Processing 23500/61682\n",
      "Processing 24000/61682\n",
      "Processing 24500/61682\n",
      "Processing 25000/61682\n",
      "Processing 25500/61682\n",
      "Processing 26000/61682\n",
      "Processing 26500/61682\n",
      "Processing 27000/61682\n",
      "Processing 27500/61682\n",
      "Processing 28000/61682\n",
      "Processing 28500/61682\n",
      "Processing 29000/61682\n",
      "Processing 29500/61682\n",
      "Processing 30000/61682\n",
      "Processing 30500/61682\n",
      "Processing 31000/61682\n",
      "Processing 31500/61682\n",
      "Processing 32000/61682\n",
      "Processing 32500/61682\n",
      "Processing 33000/61682\n",
      "Processing 33500/61682\n",
      "Processing 34000/61682\n",
      "Processing 34500/61682\n",
      "Processing 35000/61682\n",
      "Processing 35500/61682\n",
      "Processing 36000/61682\n",
      "Processing 36500/61682\n",
      "Processing 37000/61682\n",
      "Processing 37500/61682\n",
      "Processing 38000/61682\n",
      "Processing 38500/61682\n",
      "Processing 39000/61682\n",
      "Processing 39500/61682\n",
      "Processing 40000/61682\n",
      "Processing 40500/61682\n",
      "Processing 41000/61682\n",
      "Processing 41500/61682\n",
      "Processing 42000/61682\n",
      "Processing 42500/61682\n",
      "Processing 43000/61682\n",
      "Processing 43500/61682\n",
      "Processing 44000/61682\n",
      "Processing 44500/61682\n",
      "Processing 45000/61682\n",
      "Processing 45500/61682\n",
      "Processing 46000/61682\n",
      "Processing 46500/61682\n",
      "Processing 47000/61682\n",
      "Processing 47500/61682\n",
      "Processing 48000/61682\n",
      "Processing 48500/61682\n",
      "Processing 49000/61682\n",
      "Processing 49500/61682\n",
      "Processing 50000/61682\n",
      "Processing 50500/61682\n",
      "Processing 51000/61682\n",
      "Processing 51500/61682\n",
      "Processing 52000/61682\n",
      "Processing 52500/61682\n",
      "Processing 53000/61682\n",
      "Processing 53500/61682\n",
      "Processing 54000/61682\n",
      "Processing 54500/61682\n",
      "Processing 55000/61682\n",
      "Processing 55500/61682\n",
      "Processing 56000/61682\n",
      "Processing 56500/61682\n",
      "Processing 57000/61682\n",
      "Processing 57500/61682\n",
      "Processing 58000/61682\n",
      "Processing 58500/61682\n",
      "Processing 59000/61682\n",
      "Processing 59500/61682\n",
      "Processing 60000/61682\n",
      "Processing 60500/61682\n",
      "Processing 61000/61682\n",
      "Processing 61500/61682\n",
      "GPU Memory:  1.40GB allocated, 1.61GB reserved\n",
      "Generated 61682 training examples\n",
      "Mode distribution: {'HYPE_SESSION': 55021, 'VIBE_CHECK': 6571, 'GENTLE_CHECK': 38, 'REAL_TALK': 33, 'CRISIS_SUPPORT': 19}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3:  Generating Policy Training Data\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def policy_label(state):\n",
    "    \"\"\"Determine response policy based on mental state.\"\"\"\n",
    "    if state[\"risk\"] == 2:\n",
    "        return \"CRISIS_SUPPORT\"\n",
    "    if state[\"risk\"] == 1:\n",
    "        return \"GENTLE_CHECK\"\n",
    "    if state[\"intent\"] == INTENT_MAP[\"venting\"]:\n",
    "        return \"VIBE_CHECK\"\n",
    "    if state[\"intent\"] == INTENT_MAP[\"advice\"]:\n",
    "        return \"REAL_TALK\"\n",
    "    return \"HYPE_SESSION\"\n",
    "\n",
    "# Explicitly recreate and load model before inference\n",
    "print(\"Loading best classifier model...\")\n",
    "classifier_model = ImprovedMentalStateModel(\n",
    "    NUM_EMOTIONS, NUM_INTENTS, NUM_RISK, config.CLASSIFIER_MODEL\n",
    ").to(config.DEVICE)\n",
    "\n",
    "checkpoint = torch.load(f\"{config.OUTPUT_DIR}/mental_state_model_best.pth\")\n",
    "classifier_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "classifier_model.eval()\n",
    "\n",
    "# Clean up training model\n",
    "# del model\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"Generating mental state predictions...\")\n",
    "model1_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, text in enumerate(df[\"clf_text\"]):\n",
    "        if idx % 500 == 0:\n",
    "            print(f\"Processing {idx}/{len(df)}\")\n",
    "        \n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=config.CLASSIFIER_MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    ").to(config.DEVICE)\n",
    "        \n",
    "        out = classifier_model(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "        \n",
    "        model1_outputs.append({\n",
    "            \"text\": text,\n",
    "            \"emotion\": id2label[out[\"emotion\"].argmax().item()],\n",
    "            \"intent\":  int(out[\"intent\"].argmax().item()),\n",
    "            \"risk\": int(out[\"risk\"].argmax().item()),\n",
    "            \"intensity\": float(out[\"intensity\"].item()),\n",
    "            \"confidence\": float(out[\"confidence\"].item())\n",
    "        })\n",
    "\n",
    "# Clear classifier model from memory\n",
    "del classifier_model\n",
    "clear_gpu_memory()\n",
    "print_gpu_memory()\n",
    "\n",
    "# System prompt defined separately\n",
    "SYSTEM_PROMPT = \"\"\"You're Tuesday, a supportive GenZ mental wellness companion who actually gets it. \n",
    "\n",
    "## Response Modes:\n",
    "- **VIBE_CHECK**: When someone needs to vent.  Validate without fixing.\n",
    "- **REAL_TALK**: When they want practical advice. Keep it actionable but gentle.\n",
    "- **HYPE_SESSION**: Default validation and encouragement. \n",
    "- **GENTLE_CHECK**: Medium concern situations. Warm, caring, gently suggest resources.  \n",
    "- **CRISIS_SUPPORT**: Serious situations ONLY. Direct, caring, provide specific resources.\n",
    "\n",
    "## Style Guide:  \n",
    "âœ“ Natural language:  \"tbh\", \"ngl\", \"lowkey\", \"honestly\", \"fr\" (don't overuse)\n",
    "âœ“ 1-2 emojis max when appropriate ðŸ’™\n",
    "âœ“ Short paragraphs, conversational flow\n",
    "âœ“ Acknowledge feelings before anything else\n",
    "âœ— Never minimize feelings\n",
    "âœ— No toxic positivity\"\"\"\n",
    "\n",
    "# Response templates\n",
    "import random\n",
    "\n",
    "RESPONSE_TEMPLATES = {\n",
    "    \"VIBE_CHECK\": [\n",
    "        \"I hear you, {emotion} is {intensity_word} to sit with.  It makes total sense you'd feel this way.  {empathy}\",\n",
    "        \"Honestly, that sounds really {intensity_word}. Your feelings are valid, and I'm here.  {empathy}\",\n",
    "        \"Ngl, that's a lot to carry.  Feeling {emotion} right now is completely understandable. {empathy}\",\n",
    "        \"That's {intensity_word}, fr. You don't have to have it all figured out. {empathy}\",\n",
    "    ],\n",
    "    \"REAL_TALK\": [\n",
    "        \"I get that you're feeling {emotion}. Here's the thing - {advice}\",\n",
    "        \"Okay so, {emotion} is tough.  Lowkey, something that might help:  {advice}\",\n",
    "        \"Real talk:  what you're going through is valid.  One thing to consider: {advice}\",\n",
    "        \"I hear the {emotion}.  Honestly, {advice}\",\n",
    "    ],\n",
    "    \"HYPE_SESSION\": [\n",
    "        \"Can we talk about how you're actually handling this?  {validation} That's growth.\",\n",
    "        \"Okay but {validation} The fact that you're even here shows strength ngl.\",\n",
    "        \"You're being hard on yourself rn. What I see:  {validation}\",\n",
    "        \"Honestly?  {validation} That takes real courage. ðŸ’™\",\n",
    "    ],\n",
    "    \"GENTLE_CHECK\": [\n",
    "        \"I want you to know I'm hearing you, and what you're feeling matters. {gentle_support}\",\n",
    "        \"That sounds really heavy. I'm here with you.  {gentle_support}\",\n",
    "        \"I appreciate you sharing this with me. You don't have to go through this alone.  {gentle_support}\",\n",
    "    ],\n",
    "    \"CRISIS_SUPPORT\": [\n",
    "        \"I'm really concerned about what you're sharing, and I want you to get support right now. Please reach out to the 988 Suicide & Crisis Lifeline (call or text 988) or Crisis Text Line (text HOME to 741741). You deserve help.  ðŸ’™\",\n",
    "        \"What you're going through sounds incredibly hard. Please reach out to a crisis counselor right now - call/text 988 or text HOME to 741741. You matter, and help is available.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "INTENSITY_WORDS = {\n",
    "    (0.0, 0.3): \"real\",\n",
    "    (0.3, 0.5): \"tough\",\n",
    "    (0.5, 0.7): \"really hard\",\n",
    "    (0.7, 0.85): \"really heavy\",\n",
    "    (0.85, 1.0): \"overwhelming\"\n",
    "}\n",
    "\n",
    "EMPATHY_LINES = [\n",
    "    \"I'm here with you.  ðŸ’™\",\n",
    "    \"You don't have to face this alone.\",\n",
    "    \"Take your time, no rush.\",\n",
    "    \"That's a lot to hold.\",\n",
    "]\n",
    "\n",
    "ADVICE_LINES = [\n",
    "    \"Start with one small thing that feels manageable.\",\n",
    "    \"What's one tiny step that wouldn't feel overwhelming?\",\n",
    "    \"Sometimes just naming it helps.  You've already done that.\",\n",
    "    \"Be gentle with yourself while you figure this out.\",\n",
    "]\n",
    "\n",
    "VALIDATION_LINES = [\n",
    "    \"you're navigating something genuinely difficult.\",\n",
    "    \"you're showing up even when it's hard.\",\n",
    "    \"you're more capable than you're giving yourself credit for.\",\n",
    "    \"the effort you're putting in matters.\",\n",
    "]\n",
    "\n",
    "GENTLE_SUPPORT_LINES = [\n",
    "    \"If things feel too heavy, talking to someone who specializes in this could really help.  Would you be open to that?\",\n",
    "    \"You deserve support through this. Have you considered talking to a counselor?\",\n",
    "    \"There are people who want to help.  It's okay to reach out.\",\n",
    "]\n",
    "\n",
    "def get_intensity_word(intensity):\n",
    "    for (low, high), word in INTENSITY_WORDS.items():\n",
    "        if low <= intensity < high:\n",
    "            return word\n",
    "    return \"real\"\n",
    "\n",
    "# Generate training data\n",
    "policy_rows = []\n",
    "intent_names = [\"venting\", \"seeking advice\", \"looking for validation\"]\n",
    "\n",
    "for s in model1_outputs:\n",
    "    mode = policy_label(s)\n",
    "    intensity_word = get_intensity_word(s[\"intensity\"])\n",
    "    \n",
    "    # FIXED: Correct f-string formatting (no space)\n",
    "    user_message = s['text']\n",
    "    context = f\"[{s['emotion']}, intensity:{s['intensity']:.2f}, {intent_names[s['intent']]}, risk:{s['risk']}]\"\n",
    "    \n",
    "    # Generate response from template\n",
    "    if mode == \"CRISIS_SUPPORT\":\n",
    "        response = random.choice(RESPONSE_TEMPLATES[\"CRISIS_SUPPORT\"])\n",
    "    else:\n",
    "        template = random.choice(RESPONSE_TEMPLATES[mode])\n",
    "        response = template.format(\n",
    "            emotion=s[\"emotion\"],\n",
    "            intensity_word=intensity_word,\n",
    "            empathy=random.choice(EMPATHY_LINES),\n",
    "            advice=random.choice(ADVICE_LINES),\n",
    "            validation=random.choice(VALIDATION_LINES),\n",
    "            gentle_support=random.choice(GENTLE_SUPPORT_LINES)\n",
    "        )\n",
    "    \n",
    "    policy_rows.append({\n",
    "        \"user\":  user_message,\n",
    "        \"context\": context,\n",
    "        \"mode\": mode,\n",
    "        \"response\":  response,\n",
    "        \"emotion\": s[\"emotion\"]\n",
    "    })\n",
    "\n",
    "print(f\"Generated {len(policy_rows)} training examples\")\n",
    "print(f\"Mode distribution: {pd.Series([r['mode'] for r in policy_rows]).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2d78d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Could not import DataCollatorForCompletionOnlyLM from trl. Will use custom collator.\n",
      "\n",
      "================================================================================\n",
      "PART 4: Fine-tuning Phi-3.5 Mini (Optimized for 12GB VRAM)\n",
      "================================================================================\n",
      "GPU Memory:  1.40GB allocated, 1.61GB reserved\n",
      "Loading Phi-3.5 Mini with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attention 2 not available, using default attention: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory:  4.06GB allocated, 5.28GB reserved\n",
      "trainable params: 8,912,896 || all params: 3,829,992,448 || trainable%: 0.2327\n",
      "GPU Memory:  4.09GB allocated, 5.32GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61682/61682 [00:02<00:00, 26365.87 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset manually...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61682/61682 [00:09<00:00, 6534.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using custom CustomCompletionOnlyCollator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61682/61682 [00:00<00:00, 670960.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Phi-3.5 GenZ Therapist...\n",
      "GPU Memory:  4.08GB allocated, 5.32GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23133' max='23133' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23133/23133 26:16:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.602600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>4.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.156600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>0.136400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>0.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>0.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>0.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>0.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>0.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3025</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3175</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3225</td>\n",
       "      <td>0.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3275</td>\n",
       "      <td>0.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3325</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3375</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3425</td>\n",
       "      <td>0.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3475</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3525</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3575</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3625</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3725</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3775</td>\n",
       "      <td>0.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3825</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3875</td>\n",
       "      <td>0.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3925</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3975</td>\n",
       "      <td>0.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4025</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4075</td>\n",
       "      <td>0.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4125</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4175</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4225</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4275</td>\n",
       "      <td>0.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4325</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4375</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4425</td>\n",
       "      <td>0.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4475</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4525</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4575</td>\n",
       "      <td>0.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4625</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4675</td>\n",
       "      <td>0.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4725</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4775</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4825</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4875</td>\n",
       "      <td>0.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4925</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4975</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5025</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5075</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5125</td>\n",
       "      <td>0.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5175</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5225</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5275</td>\n",
       "      <td>0.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5325</td>\n",
       "      <td>0.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5375</td>\n",
       "      <td>0.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5425</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5475</td>\n",
       "      <td>0.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5525</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5575</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5625</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5675</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5725</td>\n",
       "      <td>0.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5775</td>\n",
       "      <td>0.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5825</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5875</td>\n",
       "      <td>0.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5925</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5975</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6025</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6075</td>\n",
       "      <td>0.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6125</td>\n",
       "      <td>0.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6175</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6225</td>\n",
       "      <td>0.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6275</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6325</td>\n",
       "      <td>0.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6375</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6425</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6475</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6525</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6575</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6625</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6675</td>\n",
       "      <td>0.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6725</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6775</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6825</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6875</td>\n",
       "      <td>0.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6925</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6975</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7025</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7075</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7125</td>\n",
       "      <td>0.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7175</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7225</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7275</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7325</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7375</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7425</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7475</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7525</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7575</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7625</td>\n",
       "      <td>0.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7675</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7725</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7775</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7825</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7875</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7925</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7975</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8025</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8075</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8125</td>\n",
       "      <td>0.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8175</td>\n",
       "      <td>0.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8225</td>\n",
       "      <td>0.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8275</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8325</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8375</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8425</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8475</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8525</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8575</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8625</td>\n",
       "      <td>0.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8675</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8725</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8775</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8825</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8875</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8925</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8975</td>\n",
       "      <td>0.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9025</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9075</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9125</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9175</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9225</td>\n",
       "      <td>0.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9275</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9325</td>\n",
       "      <td>0.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9375</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9425</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9475</td>\n",
       "      <td>0.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9525</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9575</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9625</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9675</td>\n",
       "      <td>0.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9725</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9775</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9825</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9875</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9925</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9975</td>\n",
       "      <td>0.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10025</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>0.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10075</td>\n",
       "      <td>0.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10125</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10175</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10225</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10275</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10325</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10375</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10425</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10475</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10525</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>0.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10575</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10625</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>0.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10675</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10725</td>\n",
       "      <td>0.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10775</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10825</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10875</td>\n",
       "      <td>0.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10925</td>\n",
       "      <td>0.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10975</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11025</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11075</td>\n",
       "      <td>0.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11125</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11175</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11225</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11275</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11325</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11375</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11425</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11475</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11525</td>\n",
       "      <td>0.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>0.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11575</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11625</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11675</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11725</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11775</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11825</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11875</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11925</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11975</td>\n",
       "      <td>0.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12025</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12075</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12125</td>\n",
       "      <td>0.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12175</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12225</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12275</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12325</td>\n",
       "      <td>0.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>0.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12375</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12425</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12475</td>\n",
       "      <td>0.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12525</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12575</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12625</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12675</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12725</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12775</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12825</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>0.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12875</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12925</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12975</td>\n",
       "      <td>0.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13025</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>0.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13075</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13125</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13175</td>\n",
       "      <td>0.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13225</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13275</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13325</td>\n",
       "      <td>0.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13375</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13425</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13475</td>\n",
       "      <td>0.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13525</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13575</td>\n",
       "      <td>0.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13625</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13675</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13725</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13775</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13825</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13875</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13925</td>\n",
       "      <td>0.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13975</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14025</td>\n",
       "      <td>0.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14075</td>\n",
       "      <td>0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14125</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14175</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14225</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14275</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14325</td>\n",
       "      <td>0.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>0.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14375</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14425</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14475</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14525</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>0.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14575</td>\n",
       "      <td>0.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14625</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14675</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14725</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14775</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14825</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14875</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14925</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14975</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15025</td>\n",
       "      <td>0.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15075</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15125</td>\n",
       "      <td>0.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15175</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15225</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>0.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15275</td>\n",
       "      <td>0.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15325</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15375</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15425</td>\n",
       "      <td>0.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15450</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15475</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15525</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15550</td>\n",
       "      <td>0.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15575</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15625</td>\n",
       "      <td>0.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15675</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15725</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15775</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15825</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15850</td>\n",
       "      <td>0.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15875</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15925</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15950</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15975</td>\n",
       "      <td>0.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16025</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16050</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16075</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16125</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16150</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16175</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16225</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16275</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16325</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16350</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16375</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16425</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16450</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16475</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16525</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16550</td>\n",
       "      <td>0.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16575</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16625</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16650</td>\n",
       "      <td>0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16675</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16725</td>\n",
       "      <td>0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16775</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16825</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16850</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16875</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16925</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16950</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16975</td>\n",
       "      <td>0.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17025</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17050</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17075</td>\n",
       "      <td>0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17125</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17150</td>\n",
       "      <td>0.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17175</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17225</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17275</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17325</td>\n",
       "      <td>0.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17350</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17375</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17425</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17450</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17475</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17525</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17575</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17625</td>\n",
       "      <td>0.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>0.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17675</td>\n",
       "      <td>0.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17725</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17775</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17825</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17850</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17875</td>\n",
       "      <td>0.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17925</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17950</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17975</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18025</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18050</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18075</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18125</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18150</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18175</td>\n",
       "      <td>0.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18225</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18275</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18325</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18350</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18375</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18425</td>\n",
       "      <td>0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18450</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18475</td>\n",
       "      <td>0.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18525</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18550</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18575</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18625</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18650</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18675</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18725</td>\n",
       "      <td>0.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18775</td>\n",
       "      <td>0.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18825</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18850</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18875</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18925</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18950</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18975</td>\n",
       "      <td>0.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19025</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19050</td>\n",
       "      <td>0.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19075</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19125</td>\n",
       "      <td>0.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19150</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19175</td>\n",
       "      <td>0.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19225</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>0.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19275</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19325</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19350</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19375</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19425</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19450</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19475</td>\n",
       "      <td>0.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19525</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19575</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19625</td>\n",
       "      <td>0.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19650</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19675</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19725</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19775</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19825</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19850</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19875</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19925</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19950</td>\n",
       "      <td>0.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19975</td>\n",
       "      <td>0.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20025</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20050</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20075</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20125</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20150</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20175</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20225</td>\n",
       "      <td>0.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20275</td>\n",
       "      <td>0.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20325</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20350</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20375</td>\n",
       "      <td>0.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20425</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20450</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20475</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20525</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20550</td>\n",
       "      <td>0.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20575</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20625</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20650</td>\n",
       "      <td>0.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20675</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20725</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20750</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20775</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20825</td>\n",
       "      <td>0.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20850</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20875</td>\n",
       "      <td>0.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20925</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20950</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20975</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21025</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21050</td>\n",
       "      <td>0.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21075</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21125</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21150</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21175</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21225</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21250</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21275</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21325</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21350</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21375</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21425</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21450</td>\n",
       "      <td>0.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21475</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21525</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21550</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21575</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21625</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21650</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21675</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21725</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21750</td>\n",
       "      <td>0.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21775</td>\n",
       "      <td>0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21825</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21850</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21875</td>\n",
       "      <td>0.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21925</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21950</td>\n",
       "      <td>0.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21975</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22025</td>\n",
       "      <td>0.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22050</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22075</td>\n",
       "      <td>0.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22125</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22150</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22175</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22225</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22250</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22275</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22325</td>\n",
       "      <td>0.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22350</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22375</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22425</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22450</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22475</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22525</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22550</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22575</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22625</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22650</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22675</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22725</td>\n",
       "      <td>0.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22750</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22775</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22825</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22850</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22875</td>\n",
       "      <td>0.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22925</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22950</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22975</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23025</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23050</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23075</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23125</td>\n",
       "      <td>0.112700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "try:\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "except ImportError:\n",
    "    raise ImportError(\"TRL is not correctly installed. Please run: pip install trl\")\n",
    "\n",
    "try:\n",
    "    from trl import DataCollatorForCompletionOnlyLM\n",
    "except ImportError:\n",
    "    print(\"! Could not import DataCollatorForCompletionOnlyLM from trl. Will use custom collator.\")\n",
    "    DataCollatorForCompletionOnlyLM = None\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 4: Fine-tuning Phi-3.5 Mini (Optimized for 12GB VRAM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "clear_gpu_memory()\n",
    "print_gpu_memory()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading Phi-3.5 Mini with 4-bit quantization...\")\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.POLICY_MODEL,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
    "phi_tokenizer.padding_side = \"right\"\n",
    "\n",
    "try:\n",
    "    phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.POLICY_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    print(\"âœ“ Using Flash Attention 2\")\n",
    "except Exception as e:\n",
    "    print(f\"Flash Attention 2 not available, using default attention: {e}\")\n",
    "    phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.POLICY_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "phi_model = prepare_model_for_kbit_training(phi_model)\n",
    "print_gpu_memory()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config.LORA_R,\n",
    "    lora_alpha=config.LORA_ALPHA,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=config.LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "phi_model = get_peft_model(phi_model, lora_config)\n",
    "phi_model.print_trainable_parameters()\n",
    "print_gpu_memory()\n",
    "\n",
    "\n",
    "def format_chat_template(example):\n",
    "    \"\"\"Format example using Phi-3's chat template.\"\"\"\n",
    "    text = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}<|end|>\n",
    "<|user|>\n",
    "{example['user']}<|end|>\n",
    "<|assistant|>\n",
    "{example['context']} {example['mode']}\n",
    "\n",
    "{example['response']}<|end|>\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "hf_dataset = HFDataset.from_list(policy_rows)\n",
    "hf_dataset = hf_dataset.map(format_chat_template, remove_columns=hf_dataset.column_names)\n",
    "\n",
    "print(\"Tokenizing dataset manually...\")\n",
    "def tokenize_function(examples):\n",
    "    return phi_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=config.POLICY_MAX_LEN,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"] \n",
    ")\n",
    "\n",
    "training_args_kwargs = dict(\n",
    "    output_dir=f\"{config.OUTPUT_DIR}/phi35_genz_therapist\",\n",
    "    per_device_train_batch_size=config.POLICY_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.POLICY_GRAD_ACCUM,\n",
    "    learning_rate=config.POLICY_LR,\n",
    "    num_train_epochs=config.POLICY_EPOCHS,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    ")\n",
    "\n",
    "sft_signature = inspect.signature(SFTTrainer.__init__)\n",
    "use_legacy_trl = (\"tokenizer\" in sft_signature.parameters) and (SFTConfig is None)\n",
    "\n",
    "if use_legacy_trl:\n",
    "    training_args = TrainingArguments(**training_args_kwargs)\n",
    "else:\n",
    "    sft_config = SFTConfig(\n",
    "        **training_args_kwargs,\n",
    "        dataset_text_field=None, \n",
    "        max_length=config.POLICY_MAX_LEN,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "if DataCollatorForCompletionOnlyLM is not None:\n",
    "    print(\"âœ“ Using trl's DataCollatorForCompletionOnlyLM\")\n",
    "    collator = DataCollatorForCompletionOnlyLM(\n",
    "        response_template=\"<|assistant|>\",\n",
    "        tokenizer=phi_tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=8,\n",
    "    )\n",
    "else:\n",
    "    print(\"âœ“ Using custom CustomCompletionOnlyCollator\")\n",
    "    collator = CustomCompletionOnlyCollator(\n",
    "        tokenizer=phi_tokenizer,\n",
    "        response_template=\"<|assistant|>\",\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "if use_legacy_trl:\n",
    "    trainer = SFTTrainer(\n",
    "        model=phi_model,\n",
    "        train_dataset=tokenized_dataset, \n",
    "        args=training_args,\n",
    "        tokenizer=phi_tokenizer,\n",
    "        dataset_text_field=None, \n",
    "        max_seq_length=config.POLICY_MAX_LEN,\n",
    "        data_collator=collator,\n",
    "        packing=False,\n",
    "    )\n",
    "else:\n",
    "    trainer = SFTTrainer(\n",
    "        model=phi_model,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        args=sft_config,\n",
    "        processing_class=phi_tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining Phi-3.5 GenZ Therapist...\")\n",
    "print_gpu_memory()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(f\"{config.OUTPUT_DIR}/phi35_genz_therapist_final\")\n",
    "phi_tokenizer.save_pretrained(f\"{config.OUTPUT_DIR}/phi35_genz_therapist_final\")\n",
    "\n",
    "with open(f\"{config.OUTPUT_DIR}/phi35_genz_therapist_final/system_prompt.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e875b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 5: Creating Unified Inference Pipeline\n",
      "================================================================================\n",
      "âœ“ Saved inference pipeline to ./models/tuesday_bot.py\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 5: Creating Unified Inference Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "inference_code = '''\"\"\"\n",
    "Tuesday Bot - GenZ Mental Wellness Companion\n",
    "Inference Pipeline\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "\n",
    "class ImprovedMentalStateModel(nn.Module):\n",
    "    \"\"\"Multi-task mental state classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_emotions, num_intents, num_risk, model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        h = self.encoder.config.hidden_size\n",
    "        \n",
    "        self.shared1 = nn.Linear(h, h)\n",
    "        self.shared2 = nn.Linear(h, h)\n",
    "        self.layer_norm = nn.LayerNorm(h)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.emotion_hidden = nn.Linear(h, h // 2)\n",
    "        self.emotion_head = nn.Linear(h // 2, num_emotions)\n",
    "        self.intent_hidden = nn.Linear(h, h // 4)\n",
    "        self.intent_head = nn.Linear(h // 4, num_intents)\n",
    "        self.risk_hidden = nn.Linear(h, h // 4)\n",
    "        self.risk_head = nn.Linear(h // 4, num_risk)\n",
    "        self.intensity_hidden = nn.Linear(h, h // 4)\n",
    "        self.intensity_head = nn.Linear(h // 4, 1)\n",
    "        self.confidence_head = nn.Linear(h, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        hidden = out.last_hidden_state\n",
    "        attention_weights = attention_mask.unsqueeze(-1).float()\n",
    "        pooled = (hidden * attention_weights).sum(dim=1) / attention_weights.sum(dim=1).clamp(min=1e-9)\n",
    "        \n",
    "        z = self.dropout(F.gelu(self.shared1(pooled)))\n",
    "        z = self.layer_norm(z + self.dropout(F.gelu(self.shared2(z))))\n",
    "        \n",
    "        emotion_h = self.dropout(F.gelu(self.emotion_hidden(z)))\n",
    "        intent_h = self.dropout(F.gelu(self.intent_hidden(z)))\n",
    "        risk_h = self.dropout(F.gelu(self.risk_hidden(z)))\n",
    "        intensity_h = self.dropout(F.gelu(self.intensity_hidden(z)))\n",
    "        \n",
    "        return {\n",
    "            \"emotion\": self.emotion_head(emotion_h),\n",
    "            \"intent\": self.intent_head(intent_h),\n",
    "            \"risk\": self.risk_head(risk_h),\n",
    "            \"intensity\": torch.sigmoid(self.intensity_head(intensity_h)).squeeze(-1),\n",
    "            \"confidence\": torch.sigmoid(self.confidence_head(z)).squeeze(-1),\n",
    "        }\n",
    "\n",
    "\n",
    "class TuesdayBot:\n",
    "    \"\"\"Unified inference pipeline for the GenZ Therapist Bot.\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dir=\"./models\", device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models_dir = models_dir\n",
    "        \n",
    "        # Load system prompt\n",
    "        prompt_path = f\"{models_dir}/phi35_genz_therapist_final/system_prompt.txt\"\n",
    "        if os.path.exists(prompt_path):\n",
    "            with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.system_prompt = f.read()\n",
    "        else:\n",
    "            self.system_prompt = \"You're Tuesday, a supportive GenZ mental wellness companion.\"\n",
    "        \n",
    "        self._load_classifier()\n",
    "        self._load_generator()\n",
    "        \n",
    "        self.intent_names = [\"venting\", \"seeking advice\", \"looking for validation\"]\n",
    "        \n",
    "    def _load_classifier(self):\n",
    "        \"\"\"Load the mental state classification model.\"\"\"\n",
    "        print(\"Loading mental state classifier...\")\n",
    "        checkpoint = torch.load(\n",
    "            f\"{self.models_dir}/mental_state_model_best.pth\",\n",
    "            map_location=self.device\n",
    "        )\n",
    "        cfg = checkpoint['config']\n",
    "        \n",
    "        self.classifier_tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])\n",
    "        \n",
    "        self.classifier = ImprovedMentalStateModel(\n",
    "            cfg['num_emotions'], cfg['num_intents'], cfg['num_risk'], cfg['model_name']\n",
    "        )\n",
    "        self.classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.classifier.to(self.device)\n",
    "        self.classifier.eval()\n",
    "        \n",
    "        self.id2label = checkpoint['id2label']\n",
    "        print(f\"  âœ“ Loaded classifier with {cfg['num_emotions']} emotions\")\n",
    "        \n",
    "    def _load_generator(self):\n",
    "        \"\"\"Load the fine-tuned response generator.\"\"\"\n",
    "        print(\"Loading response generator...\")\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        \n",
    "        model_path = f\"{self.models_dir}/phi35_genz_therapist_final\"\n",
    "        \n",
    "        self.gen_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.gen_tokenizer.pad_token = self.gen_tokenizer.eos_token\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"microsoft/Phi-3.5-mini-instruct\",\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.generator = PeftModel.from_pretrained(base_model, model_path)\n",
    "        self.generator.eval()\n",
    "        print(\"  âœ“ Loaded response generator\")\n",
    "    \n",
    "    def analyze_mental_state(self, text):\n",
    "        \"\"\"Analyze the mental state from user input.\"\"\"\n",
    "        enc = self.classifier_tokenizer(\n",
    "            text.lower().strip(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = self.classifier(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "        \n",
    "        return {\n",
    "            \"emotion\": self.id2label[out[\"emotion\"].argmax().item()],\n",
    "            \"intent\": int(out[\"intent\"].argmax().item()),\n",
    "            \"risk\": int(out[\"risk\"].argmax().item()),\n",
    "            \"intensity\": float(out[\"intensity\"].item()),\n",
    "            \"confidence\": float(out[\"confidence\"].item())\n",
    "        }\n",
    "    \n",
    "    def get_response_mode(self, state):\n",
    "        \"\"\"Determine the appropriate response mode.\"\"\"\n",
    "        if state[\"risk\"] == 2:\n",
    "            return \"CRISIS_SUPPORT\"\n",
    "        if state[\"risk\"] == 1:\n",
    "            return \"GENTLE_CHECK\"\n",
    "        if state[\"intent\"] == 0:\n",
    "            return \"VIBE_CHECK\"\n",
    "        if state[\"intent\"] == 1:\n",
    "            return \"REAL_TALK\"\n",
    "        return \"HYPE_SESSION\"\n",
    "    \n",
    "    def generate_response(self, user_input, max_new_tokens=256, temperature=0.7):\n",
    "        \"\"\"Generate a supportive response.\"\"\"\n",
    "        state = self.analyze_mental_state(user_input)\n",
    "        mode = self.get_response_mode(state)\n",
    "        \n",
    "        # Build prompt in Phi-3 format\n",
    "        prompt = f\"\"\"<|system|>\n",
    "{self.system_prompt}<|end|>\n",
    "<|user|>\n",
    "{user_input}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "        \n",
    "        # Add context for model (FIXED: correct f-string format)\n",
    "        context = f\"[{state['emotion']}, intensity:{state['intensity']:.2f}, {self.intent_names[state['intent']]}, risk:{state['risk']}] {mode}\\\\n\\\\n\"\n",
    "        prompt = prompt + context\n",
    "        \n",
    "        inputs = self.gen_tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.generator.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=self.gen_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        full_response = self.gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the generated part\n",
    "        if \"<|assistant|>\" in full_response:\n",
    "            response = full_response.split(\"<|assistant|>\")[-1]\n",
    "        else:\n",
    "            response = full_response\n",
    "        \n",
    "        response = response.replace(\"<|end|>\", \"\").strip()\n",
    "        \n",
    "        # Remove context prefix if present\n",
    "        if response.startswith(\"[\"):\n",
    "            bracket_end = response.find(\"]\")\n",
    "            if bracket_end != -1:\n",
    "                response = response[bracket_end + 1:].strip()\n",
    "                for m in [\"CRISIS_SUPPORT\", \"GENTLE_CHECK\", \"VIBE_CHECK\", \"REAL_TALK\", \"HYPE_SESSION\"]:\n",
    "                    if response.startswith(m):\n",
    "                        response = response[len(m):].strip()\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"mental_state\": state,\n",
    "            \"mode\": mode\n",
    "        }\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chat mode.\"\"\"\n",
    "        print(\"\\\\n\" + \"=\" * 60)\n",
    "        print(\"Tuesday Bot - GenZ Mental Wellness Companion\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        print(\"=\" * 60 + \"\\\\n\")\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\\\nTake care! Remember, you're doing better than you think.  ðŸ’™\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            result = self.generate_response(user_input)\n",
    "            print(f\"\\\\n[{result['mode']}] Tuesday:  {result['response']}\\\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    bot = TuesdayBot()\n",
    "    \n",
    "    test_inputs = [\n",
    "        \"I've been feeling so anxious about my job interview tomorrow\",\n",
    "        \"Just had the best day ever!  Got promoted! \",\n",
    "        \"I feel like nobody understands me and I'm so alone\",\n",
    "        \"What should I do about my relationship problems?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"Testing Tuesday Bot\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for text in test_inputs:\n",
    "        print(f\"\\\\nUser: {text}\")\n",
    "        result = bot.generate_response(text)\n",
    "        # FIXED: Correct f-string format\n",
    "        print(f\"State: {result['mental_state']['emotion']} (intensity: {result['mental_state']['intensity']:.2f})\")\n",
    "        print(f\"Mode: {result['mode']}\")\n",
    "        print(f\"Tuesday:  {result['response']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\\\nStarting interactive chat...\")\n",
    "    bot.chat()\n",
    "'''\n",
    "\n",
    "with open(f\"{config.OUTPUT_DIR}/tuesday_bot.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "print(f\"âœ“ Saved inference pipeline to {config.OUTPUT_DIR}/tuesday_bot.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694f7027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ¨ PIPELINE COMPLETE!  âœ¨\n",
      "================================================================================\n",
      "\n",
      "Models trained and saved to ./models/: \n",
      "1. mental_state_model_best.pth - Multi-task emotion/intent/risk classifier\n",
      "2. phi35_genz_therapist_final/ - Fine-tuned Phi-3.5 Mini response generator\n",
      "\n",
      "Bugs Fixed: \n",
      "âœ“ File path:  Removed space in \"cleaned. csv\"\n",
      "âœ“ Float formatting: Fixed all \":. 2f\" syntax (removed spaces)\n",
      "âœ“ Model scope: ImprovedMentalStateModel defined globally\n",
      "âœ“ System prompt: Using manual Phi-3 format\n",
      "âœ“ DataCollator: Custom implementation for older trl versions\n",
      "\n",
      "Memory Optimizations for RTX 4080 12GB:\n",
      "âœ“ 4-bit quantization with double quantization\n",
      "âœ“ bfloat16 compute dtype\n",
      "âœ“ Gradient checkpointing\n",
      "âœ“ 8-bit paged AdamW optimizer\n",
      "âœ“ Flash Attention 2 (if available)\n",
      "\n",
      "Usage:\n",
      "    from tuesday_bot import TuesdayBot\n",
      "    bot = TuesdayBot(models_dir=\"./models\")\n",
      "    result = bot.generate_response(\"I'm feeling anxious today\")\n",
      "    print(result[\"response\"])\n",
      "\n",
      "    # Or interactive chat: \n",
      "    bot.chat()\n",
      "\n",
      "GPU Memory:  4.09GB allocated, 5.34GB reserved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ¨ PIPELINE COMPLETE!  âœ¨\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "Models trained and saved to {config.OUTPUT_DIR}/: \n",
    "1. mental_state_model_best.pth - Multi-task emotion/intent/risk classifier\n",
    "2. phi35_genz_therapist_final/ - Fine-tuned Phi-3.5 Mini response generator\n",
    "\n",
    "Bugs Fixed: \n",
    "âœ“ File path:  Removed space in \"cleaned. csv\"\n",
    "âœ“ Float formatting: Fixed all \":. 2f\" syntax (removed spaces)\n",
    "âœ“ Model scope: ImprovedMentalStateModel defined globally\n",
    "âœ“ System prompt: Using manual Phi-3 format\n",
    "âœ“ DataCollator: Custom implementation for older trl versions\n",
    "\n",
    "Memory Optimizations for RTX 4080 12GB:\n",
    "âœ“ 4-bit quantization with double quantization\n",
    "âœ“ bfloat16 compute dtype\n",
    "âœ“ Gradient checkpointing\n",
    "âœ“ 8-bit paged AdamW optimizer\n",
    "âœ“ Flash Attention 2 (if available)\n",
    "\n",
    "Usage:\n",
    "    from tuesday_bot import TuesdayBot\n",
    "    bot = TuesdayBot(models_dir=\"{config.OUTPUT_DIR}\")\n",
    "    result = bot.generate_response(\"I'm feeling anxious today\")\n",
    "    print(result[\"response\"])\n",
    "    \n",
    "    # Or interactive chat: \n",
    "    bot.chat()\n",
    "\"\"\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
